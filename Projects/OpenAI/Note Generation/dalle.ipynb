{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting dalle2-pytorch\n",
      "  Downloading dalle2_pytorch-0.3.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting rotary-embedding-torch\n",
      "  Downloading rotary_embedding_torch-0.1.5-py3-none-any.whl (4.1 kB)\n",
      "Collecting torch>=1.10\n",
      "  Downloading torch-1.11.0-cp38-none-macosx_10_9_x86_64.whl (129.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 MB\u001b[0m \u001b[31m985.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from dalle2-pytorch) (8.0.3)\n",
      "Collecting resize-right>=0.0.2\n",
      "  Downloading resize_right-0.0.2-py3-none-any.whl (8.9 kB)\n",
      "Requirement already satisfied: pillow in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from dalle2-pytorch) (9.0.0)\n",
      "Collecting coca-pytorch>=0.0.5\n",
      "  Downloading CoCa_pytorch-0.0.5-py3-none-any.whl (6.4 kB)\n",
      "Collecting einops>=0.4\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Collecting webdataset>=0.2.5\n",
      "  Downloading webdataset-0.2.5-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2022.1.0\n",
      "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.1/136.1 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from dalle2-pytorch) (1.22.0)\n",
      "Collecting kornia>=0.5.4\n",
      "  Downloading kornia-0.6.5-py2.py3-none-any.whl (512 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.8/512.8 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting x-clip>=0.4.4\n",
      "  Downloading x_clip-0.5.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting youtokentome\n",
      "  Downloading youtokentome-1.0.6-cp38-cp38-macosx_10_14_x86_64.whl (164 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting einops-exts>=0.0.3\n",
      "  Downloading einops_exts-0.0.3-py3-none-any.whl (3.8 kB)\n",
      "Collecting vector-quantize-pytorch\n",
      "  Downloading vector_quantize_pytorch-0.6.0-py3-none-any.whl (6.5 kB)\n",
      "Collecting embedding-reader\n",
      "  Downloading embedding_reader-1.4.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: tqdm in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from dalle2-pytorch) (4.64.0)\n",
      "Collecting clip-anytorch\n",
      "  Downloading clip_anytorch-2.4.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp38-cp38-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from kornia>=0.5.4->dalle2-pytorch) (21.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from torch>=1.10->dalle2-pytorch) (4.0.1)\n",
      "Collecting braceexpand\n",
      "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: pyyaml in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from webdataset>=0.2.5->dalle2-pytorch) (6.0)\n",
      "Requirement already satisfied: regex in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from x-clip>=0.4.4->dalle2-pytorch) (2022.4.24)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow<8,>=6.0.1\n",
      "  Downloading pyarrow-7.0.0-cp38-cp38-macosx_10_13_x86_64.whl (20.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas<2,>=1.1.5 in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from embedding-reader->dalle2-pytorch) (1.4.0)\n",
      "Requirement already satisfied: requests in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from torchvision->dalle2-pytorch) (2.27.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from pandas<2,>=1.1.5->embedding-reader->dalle2-pytorch) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from pandas<2,>=1.1.5->embedding-reader->dalle2-pytorch) (2.8.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from ftfy->x-clip>=0.4.4->dalle2-pytorch) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from packaging->kornia>=0.5.4->dalle2-pytorch) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from requests->torchvision->dalle2-pytorch) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from requests->torchvision->dalle2-pytorch) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from requests->torchvision->dalle2-pytorch) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages (from requests->torchvision->dalle2-pytorch) (1.26.8)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas<2,>=1.1.5->embedding-reader->dalle2-pytorch) (1.15.0)\n",
      "Installing collected packages: resize-right, einops, braceexpand, youtokentome, webdataset, torch, pyarrow, ftfy, fsspec, einops-exts, vector-quantize-pytorch, torchvision, rotary-embedding-torch, kornia, coca-pytorch, x-clip, embedding-reader, clip-anytorch, dalle2-pytorch\n",
      "\u001b[33m  WARNING: The script yttm is installed in '/Users/joshsteinbecker/Library/Python/3.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/Users/joshsteinbecker/Library/Python/3.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script plasma_store is installed in '/Users/joshsteinbecker/Library/Python/3.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script ftfy is installed in '/Users/joshsteinbecker/Library/Python/3.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts dalle2_pytorch and dream are installed in '/Users/joshsteinbecker/Library/Python/3.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed braceexpand-0.1.7 clip-anytorch-2.4.0 coca-pytorch-0.0.5 dalle2-pytorch-0.3.2 einops-0.4.1 einops-exts-0.0.3 embedding-reader-1.4.1 fsspec-2022.3.0 ftfy-6.1.1 kornia-0.6.5 pyarrow-7.0.0 resize-right-0.0.2 rotary-embedding-torch-0.1.5 torch-1.11.0 torchvision-0.12.0 vector-quantize-pytorch-0.6.0 webdataset-0.2.5 x-clip-0.5.1 youtokentome-1.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip3 install dalle2-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note Generation/dalle.ipynb Cell 2'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdalle2_pytorch\u001b[39;00m \u001b[39mimport\u001b[39;00m DiffusionPriorNetwork, DiffusionPrior\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=3'>4</a>\u001b[0m \u001b[39m# setup prior network, which contains an autoregressive transformer\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=5'>6</a>\u001b[0m prior_network \u001b[39m=\u001b[39m DiffusionPriorNetwork(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=6'>7</a>\u001b[0m     dim \u001b[39m=\u001b[39;49m \u001b[39m512\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=7'>8</a>\u001b[0m     depth \u001b[39m=\u001b[39;49m \u001b[39m6\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=8'>9</a>\u001b[0m     dim_head \u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=9'>10</a>\u001b[0m     heads \u001b[39m=\u001b[39;49m \u001b[39m8\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=10'>11</a>\u001b[0m )\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=12'>13</a>\u001b[0m \u001b[39m# diffusion prior network, which contains the CLIP and network (with transformer) above\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=14'>15</a>\u001b[0m diffusion_prior \u001b[39m=\u001b[39m DiffusionPrior(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=15'>16</a>\u001b[0m     net \u001b[39m=\u001b[39m prior_network,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=16'>17</a>\u001b[0m     image_embed_dim \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m,               \u001b[39m# this needs to be set\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=19'>20</a>\u001b[0m     condition_on_text_encodings \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# this probably should be true, but just to get Laion started\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshsteinbecker/jts_project/Projects/OpenAI/Note%20Generation/dalle.ipynb#ch0000001?line=20'>21</a>\u001b[0m )\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:688\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=670'>671</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=671'>672</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=672'>673</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=673'>674</a>\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=685'>686</a>\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=686'>687</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=687'>688</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 578 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=596'>597</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=597'>598</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=598'>599</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=599'>600</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=600'>601</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=601'>602</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=602'>603</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:688\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=670'>671</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=671'>672</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=672'>673</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=673'>674</a>\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=685'>686</a>\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=686'>687</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=687'>688</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/cuda/__init__.py:210\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/cuda/__init__.py?line=205'>206</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/cuda/__init__.py?line=206'>207</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/cuda/__init__.py?line=207'>208</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/cuda/__init__.py?line=208'>209</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/cuda/__init__.py?line=209'>210</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/cuda/__init__.py?line=210'>211</a>\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/cuda/__init__.py?line=211'>212</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/joshsteinbecker/Library/Python/3.8/lib/python/site-packages/torch/cuda/__init__.py?line=212'>213</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dalle2_pytorch import DiffusionPriorNetwork, DiffusionPrior\n",
    "\n",
    "# setup prior network, which contains an autoregressive transformer\n",
    "\n",
    "prior_network = DiffusionPriorNetwork(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ").cuda()\n",
    "\n",
    "# diffusion prior network, which contains the CLIP and network (with transformer) above\n",
    "\n",
    "diffusion_prior = DiffusionPrior(\n",
    "    net = prior_network,\n",
    "    image_embed_dim = 512,               # this needs to be set\n",
    "    timesteps = 100,\n",
    "    cond_drop_prob = 0.2,\n",
    "    condition_on_text_encodings = False  # this probably should be true, but just to get Laion started\n",
    ").cuda()\n",
    "\n",
    "# mock data\n",
    "\n",
    "text = torch.randint(0, 49408, (4, 256)).cuda()\n",
    "images = torch.randn(4, 3, 256, 256).cuda()\n",
    "\n",
    "# precompute the text and image embeddings\n",
    "# here using the diffusion prior class, but could be done with CLIP alone\n",
    "\n",
    "clip_image_embeds = torch.randn(4, 512).cuda()\n",
    "clip_text_embeds = torch.randn(4, 512).cuda()\n",
    "\n",
    "# feed text and images into diffusion prior network\n",
    "\n",
    "loss = diffusion_prior(\n",
    "    text_embed = clip_text_embeds,\n",
    "    image_embed = clip_image_embeds\n",
    ")\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# do the above for many many many steps\n",
    "# now the diffusion prior can generate image embeddings from the text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
